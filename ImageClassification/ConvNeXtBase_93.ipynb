{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning - ConvNeXtBase\n",
        "This notebook includes experiments done for the purpose of the first homework due for the Artificial Neural Networks & Deep Learning course at Politecnico di Milano. Here the focus is on applying _Transfer Learning_ using _ConvNeXtBase_ as pre-trained network.\n",
        "\n"
      ],
      "metadata": {
        "id": "c8BXqYphxioF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "The first homework asked to design and train a CNN to make it perform a binary image classification task, starting from a labelled dataset of healhty and unhealthy plants. After being trained, the network should have been able to make predictions on the health status of a plant, given as input a picture of it."
      ],
      "metadata": {
        "id": "zz1vFFQ2xwME"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj7uUR5Gkoxo"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Connect to Google Drive\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMj8q0EogqIu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2023-2024] AN2DL/Homework1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTJYWGZmk1DU"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SQ-CkdvMjgei"
      },
      "outputs": [],
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Totzj-WmtYuB",
        "outputId": "595cff11-761d-4e2d-d44b-cbb16e990ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Fho5a1IYlyR4"
      },
      "outputs": [],
      "source": [
        "# Import other libraries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from sklearn.utils import class_weight\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9zmpIigk5sO"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the public dataset\n",
        "public_data = np.load('public_data.npz', allow_pickle=True)\n",
        "\n",
        "# Get the 3-dimensional numpy array of samples\n",
        "X_train_val = public_data['data']\n",
        "\n",
        "# Get the 1-dimensional numpy array of labels\n",
        "y_train_val = public_data['labels']\n",
        "\n",
        "# Expand the dimension of labels\n",
        "y_train_val = np.expand_dims(y_train_val, axis=-1)"
      ],
      "metadata": {
        "id": "38Sz1rsKx9rW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect data"
      ],
      "metadata": {
        "id": "cyBD585kx_bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Shape:\", X_train_val.shape)\n",
        "print(\"Training Validation Data Shape\", y_train_val.shape)"
      ],
      "metadata": {
        "id": "fsG2jXFUyFyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data to the range [0, 1]\n",
        "X_train_val = X_train_val.astype(\"float32\")/255."
      ],
      "metadata": {
        "id": "P-u7B9s_yHbT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of images from the training-validation dataset\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
        "\n",
        "# Iterate through the selected number of images\n",
        "for i in range(num_img):\n",
        "    idx = np.random.randint(len(X_train_val))\n",
        "    ax = axes[i % num_img]\n",
        "    ax.imshow(X_train_val[idx])\n",
        "    ax.set_title('{}'.format(y_train_val[idx][0]))\n",
        "\n",
        "# Adjust layout and display the images\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7W3NPOuRyJbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove outliers\n",
        "To remove outliers from the dataset, a clustering algorithm is applied.\n",
        "\n",
        "\n",
        "More in detail, _DBSCAN_ (Density-Based Spatial Clustering of Applications with Noise) is the algorithm responsible of identifying clusters in the space of data points.\n",
        "\n",
        "\n",
        "_DBSCAN_ makes use of two parameters:\n",
        "\n",
        "*   `eps`: the maximum distance between a couple of data points in order for them to be considered part of the same neighbourhood\n",
        "*   `min_samples`: the minimum number of data points required to form a dense region\n"
      ],
      "metadata": {
        "id": "-wLgKBzuyLhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ratw4Ab0g8Gz"
      },
      "outputs": [],
      "source": [
        "# Flatten training-validation aset\n",
        "X_flattened = np.array(X_train_val).reshape(len(X_train_val), -1)\n",
        "\n",
        "# Standardize the flattened data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X_flattened)\n",
        "\n",
        "# Apply DBSCAN clustering algorithm\n",
        "scan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = scan.fit_predict(X_normalized)\n",
        "\n",
        "# Count the occurrences of each cluster label\n",
        "cluster_count = Counter(labels)\n",
        "\n",
        "# Create a dictionary to store images and labels for each cluster\n",
        "clustered_images = defaultdict(list)\n",
        "\n",
        "# Iterate over all points and store images and labels based on their cluster\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_images[label].append((X_train_val[i], y_train_val[i]))\n",
        "\n",
        "# Identify the label of the dominant cluster\n",
        "plant_cluster_label = max(cluster_count, key=cluster_count.get)\n",
        "\n",
        "# Filter images belonging to the dominant cluster label\n",
        "filtered_images = [image for i, image in enumerate(X_train_val) if labels[i] == plant_cluster_label]\n",
        "\n",
        "# Filter labels corresponding to the filtered images\n",
        "filtered_labels = [label for i, label in enumerate(y_train_val) if labels[i] == plant_cluster_label]\n",
        "\n",
        "# Update X_train_val and y_train_val with filtered images and labels, respectively\n",
        "X_train_val = np.array(filtered_images)\n",
        "y_train_val = np.array(filtered_labels)\n",
        "\n",
        "# Save other clusters\n",
        "other_clusters = {label: [(image, label) for image, label in clustered_images[label]]\n",
        "                  for label in clustered_images if label != plant_cluster_label}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples per outlier cluster\n",
        "for label, count in cluster_count.items():\n",
        "  if label == plant_cluster_label:\n",
        "    print(f\"Cluster of plants: {count} samples\")\n",
        "  else:\n",
        "    print(f\"Cluster {1+label} of outliers: {count} samples\")\n",
        "\n",
        "# Print the shape of the cleaned training-validation dataset\n",
        "print(\"Training Data Shape:\", X_train_val.shape)\n",
        "print(\"Training Validation Data Shape\", y_train_val.shape)"
      ],
      "metadata": {
        "id": "DhbKM2soyTdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of images from the cleaned training-validation dataset\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
        "\n",
        "# Iterate through the selected number of images\n",
        "for i in range(num_img):\n",
        "    idx = np.random.randint(len(X_train_val))\n",
        "    ax = axes[i % num_img]\n",
        "    ax.imshow(X_train_val[idx])\n",
        "    ax.set_title('{}'.format(y_train_val[idx][0]))\n",
        "\n",
        "# Adjust layout and display the images\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nbGB0PDhyWle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 5 samples from outliers cluster 1 and 5 samples from outliers cluster 2\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "\n",
        "# Display 5 samples from outliers cluster 1\n",
        "outliers_cluster_1_data = other_clusters.get(0, [])[:5]  # Get data for outliers cluster 1\n",
        "for i, (image, label) in enumerate(outliers_cluster_1_data):\n",
        "    ax = axes[0, i]\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(f'Outliers Cluster 1')\n",
        "\n",
        "# Display 5 samples from outliers cluster 2\n",
        "outliers_cluster_2_data = other_clusters.get(1, [])[:5]  # Get data for outliers cluster 2\n",
        "for i, (image, label) in enumerate(outliers_cluster_2_data):\n",
        "    ax = axes[1, i]\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(f'Outliers Cluster 2')\n",
        "\n",
        "# Adjust layout and display the images\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NolPB-iVybN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iWZk3ERk8DM"
      },
      "source": [
        "## Generate test set\n",
        "A test set is conditionally generated, depending on the value of a boolean variable.\n",
        "\n",
        "Indeed the test set is meant to be used only locally. This is to accomplish an understanding of the model's perofrmance, even without having to test it on CodaLab.\n",
        "\n",
        "However the local test set is meant to be removed before training the model to be uploaded on CodaLab, in order to let it learn from as many samples as possible, being the input dataset limited in size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2OlLxiLwQq6"
      },
      "outputs": [],
      "source": [
        "is_generate_test_set_on = False\n",
        "\n",
        "if is_generate_test_set_on:\n",
        "  # Split the cleaned dataset into training-validation and test sets\n",
        "  X_train_val, X_test, y_train_val, y_test = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=0.175, stratify=y_train_val)\n",
        "  # Split the train-validation set into training and validation sets\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=y_train_val)\n",
        "\n",
        "  print(\"Test Data Shape:\", X_test.shape)\n",
        "  print(\"Test Label Shape:\", y_test.shape)\n",
        "\n",
        "  # Map 'healthy' values of labels to '1' and 'unhealthy' to '0'\n",
        "  y_test = (y_test == 'unhealthy').astype(int)\n",
        "\n",
        "else:\n",
        "  # Split the clean dataset into training and validation sets\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=0.175, stratify=y_train_val)\n",
        "\n",
        "print(\"Training Data Shape:\", X_train.shape)\n",
        "print(\"Training Label Shape:\", y_train.shape)\n",
        "print(\"Validation Data Shape:\", X_val.shape)\n",
        "print(\"Validation Label Shape:\", y_val.shape)\n",
        "\n",
        "# Map 'healthy' values of labels to '1' and 'unhealthy' to '0'\n",
        "y_val = (y_val == 'unhealthy').astype(int)\n",
        "y_train = (y_train == 'unhealthy').astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduce class weights"
      ],
      "metadata": {
        "id": "4_tOqzK_yzDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhRTQCRMgH0V"
      },
      "outputs": [],
      "source": [
        "# Number of classes\n",
        "classes = 2\n",
        "\n",
        "# Initialize a numpy array for counting class occurences\n",
        "elements_per_class = np.zeros(2, dtype='int')\n",
        "\n",
        "# Total number of images in the train set\n",
        "total_img = y_train.size\n",
        "\n",
        "# Count the occurences of the target classes in the training set\n",
        "for i in y_train:\n",
        "  elements_per_class[i]+=1\n",
        "\n",
        "proportions = {0: elements_per_class[0]/total_img, 1:elements_per_class[1]/total_img}\n",
        "print('Proportion of classes: ', proportions)\n",
        "\n",
        "# Initialize a numpy array for class weights\n",
        "class_weights = np.zeros(2, dtype=float)\n",
        "\n",
        "# Compute classe weights for each class\n",
        "class_weights[0] = total_img/(classes*elements_per_class[0])\n",
        "class_weights[1] = total_img/(classes*elements_per_class[1])\n",
        "\n",
        "class_weights={0: class_weights[0], 1:class_weights[1]}\n",
        "print('Class weigths: ', class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "UMPmT3WHy5Sz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EnB-mth44ty",
        "outputId": "2cf8f3aa-c0ea-4aa0-d69b-e3ad11540daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 64, Epochs: 200, Learning Rate: 0.001\n",
            "Input Shape: (96, 96, 3), Output Shape: 1\n"
          ]
        }
      ],
      "source": [
        "# Define batch size, number of epochs, learning rate, input shape, and output shape\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "learning_rate = 1e-3\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "output_shape = y_train.shape[-1]\n",
        "\n",
        "# Print batch size, epochs, learning rate, input shape, and output shape\n",
        "print(f\"Batch Size: {batch_size}, Epochs: {epochs}, Learning Rate: {learning_rate}\")\n",
        "print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "-bxexUQU63Am"
      },
      "outputs": [],
      "source": [
        "net = tfk.applications.ConvNeXtBase(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "j0-8bKcFYpGj"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, output_shape, seed=seed):\n",
        "  # Initialize random seed\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "  # Define trnaformation used for data augmentation\n",
        "  preprocessing = tf.keras.Sequential([\n",
        "      tfkl.RandomFlip(\"horizontal_and_vertical\", seed = seed),\n",
        "      tfkl.RandomTranslation(0.1,0.1, seed = seed, fill_mode = \"reflect\"),\n",
        "      tfkl.RandomRotation(0.3,seed = seed, fill_mode = \"reflect\"),\n",
        "      tfkl.RandomZoom(0.2,seed = seed, fill_mode = \"reflect\")\n",
        "    ], name='preprocessing')\n",
        "\n",
        "  # Keep weigths freezed for each layer\n",
        "  net.trainable = False\n",
        "\n",
        "  input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "  # Apply data augmentation to the input\n",
        "  preprocessing = preprocessing(input_layer)\n",
        "  x = net(preprocessing)\n",
        "  x= tfkl.Dropout(0.3, seed=seed)(x)\n",
        "  output_layer = tfkl.Dense(units=output_shape, activation='sigmoid',name='Output', kernel_regularizer=tf.keras.regularizers.l2(2e-4))(x)\n",
        "\n",
        " # Create a Model connecting input and output\n",
        "  model = tfk.Model(inputs=input_layer, outputs=output_layer, name='CNN')\n",
        "\n",
        " # Compile the model\n",
        "  model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "  # Return model\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJI1yEpZb987"
      },
      "outputs": [],
      "source": [
        "model = build_model(input_shape, output_shape, seed)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "tKy6Gy2m0g9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xv2aVcq-jfa"
      },
      "outputs": [],
      "source": [
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)\n",
        "LR_reduction =  tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.9, patience=2, min_lr=1e-5, mode='max')\n",
        "\n",
        "\n",
        "# Train the model and save its history\n",
        "history = model.fit(\n",
        "    x=(X_train*255),\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    class_weight = class_weights,\n",
        "    validation_data=((X_val*255), y_val),\n",
        "    callbacks=[early_stopping,LR_reduction]\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot metrics"
      ],
      "metadata": {
        "id": "7qFeQdT80qGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viFjYp5c-seW"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmin(history['val_loss'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(np.argmin(history['val_loss']), history['val_loss'][np.argmin(history['val_loss'])], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(np.argmax(history['val_accuracy']), history['val_accuracy'][np.argmax(history['val_accuracy'])], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make inference"
      ],
      "metadata": {
        "id": "moupMjyH1kng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if is_generate_test_set_on:\n",
        "  # Predict labels for the entire test set\n",
        "  predictions = model.predict(X_test*255, verbose=0)\n",
        "\n",
        "  # Display the shape of the predictions\n",
        "  print(\"Predictions Shape:\", predictions.shape)"
      ],
      "metadata": {
        "id": "Jh2BqXc01m3i"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_generate_test_set_on:\n",
        "  # Compute the confusion matrix\n",
        "  threshold = 0.5\n",
        "  binary_prediction = tf.where(predictions > threshold, 1, 0)\n",
        "  cm = confusion_matrix(y_test, binary_prediction)\n",
        "\n",
        "  # Compute classification metrics\n",
        "  accuracy = accuracy_score(y_test, binary_prediction)\n",
        "  precision = precision_score(y_test, binary_prediction, average='macro')\n",
        "  recall = recall_score(y_test, binary_prediction, average='macro')\n",
        "  f1 = f1_score(y_test, binary_prediction, average='macro')\n",
        "\n",
        "  # Display the computed metrics\n",
        "  print('Accuracy:', accuracy.round(4))\n",
        "  print('Precision:', precision.round(4))\n",
        "  print('Recall:', recall.round(4))\n",
        "  print('F1:', f1.round(4))\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "  plt.xlabel('True labels')\n",
        "  plt.ylabel('Predicted labels')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yiIiUNMH1uzX"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "_wnLk9n80wpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "POM2BJokGUg7"
      },
      "outputs": [],
      "source": [
        "save = False\n",
        "\n",
        "if save:\n",
        "  model.save(\"13_11_23_test_convnestbase_FT_2Dense\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "b2CH-iBD07s-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "D6HWkXsAAnst"
      },
      "outputs": [],
      "source": [
        "load = False\n",
        "\n",
        "if load:\n",
        "  model = tf.keras.models.load_model('13_11_23_test_convnestbase_FT_2Dense')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning"
      ],
      "metadata": {
        "id": "1dL8dcPu1Ag6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uYTPz9GKNDR"
      },
      "outputs": [],
      "source": [
        "model.get_layer('convnext_base').trainable = True\n",
        "for i, layer in enumerate(model.get_layer('convnext_base').layers):\n",
        "   print(i, layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdkx8UuIMmtC"
      },
      "outputs": [],
      "source": [
        "# Freeze first N layers\n",
        "N = 109\n",
        "for i, layer in enumerate(model.get_layer('convnext_base').layers[:N]):\n",
        "  layer.trainable=False\n",
        "for i, layer in enumerate(model.get_layer('convnext_base').layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "UpuTq8E2M2qW"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(7e-5), metrics='accuracy')     #### MODIFIED learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTmNh0xCNhRn"
      },
      "outputs": [],
      "source": [
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
        "LR_reduction =  tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=10, min_lr=1e-6, mode='max')\n",
        "\n",
        "# Train the model and save its history\n",
        "history = model.fit(\n",
        "    x=(X_train*255),\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    class_weight = class_weights,\n",
        "    validation_data=((X_val*255), y_val),\n",
        "    callbacks=[early_stopping,LR_reduction]\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot metric fine tuning"
      ],
      "metadata": {
        "id": "DO9VjlTt1HnD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ks5IA7URTqb"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation loss\n",
        "best_epoch = np.argmin(history['val_loss'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(np.argmin(history['val_loss']), history['val_loss'][np.argmin(history['val_loss'])], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(np.argmax(history['val_accuracy']), history['val_accuracy'][np.argmax(history['val_accuracy'])], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make inference after fine tuning"
      ],
      "metadata": {
        "id": "MHWW2SyF1WND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if is_generate_test_set_on:\n",
        "  # Predict labels for the entire test set\n",
        "  predictions = model.predict(X_test*255, verbose=0)\n",
        "\n",
        "  # Display the shape of the predictions\n",
        "  print(\"Predictions Shape:\", predictions.shape)"
      ],
      "metadata": {
        "id": "GaLeWPWX1ZnG"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_generate_test_set_on:\n",
        "  # Compute the confusion matrix\n",
        "  threshold = 0.5\n",
        "  binary_prediction = tf.where(predictions > threshold, 1, 0)\n",
        "  cm = confusion_matrix(y_test, binary_prediction)\n",
        "\n",
        "  # Compute classification metrics\n",
        "  accuracy = accuracy_score(y_test, binary_prediction)\n",
        "  precision = precision_score(y_test, binary_prediction, average='macro')\n",
        "  recall = recall_score(y_test, binary_prediction, average='macro')\n",
        "  f1 = f1_score(y_test, binary_prediction, average='macro')\n",
        "\n",
        "  # Display the computed metrics\n",
        "  print('Accuracy:', accuracy.round(4))\n",
        "  print('Precision:', precision.round(4))\n",
        "  print('Recall:', recall.round(4))\n",
        "  print('F1:', f1.round(4))\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "  plt.xlabel('True labels')\n",
        "  plt.ylabel('Predicted labels')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "hdndzzMM1cc7"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model"
      ],
      "metadata": {
        "id": "NuJ85hU11MuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RQfNwZZReeHY"
      },
      "outputs": [],
      "source": [
        "save = False\n",
        "\n",
        "if save:\n",
        "  model.save('Models/ConvNeXt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CTJYWGZmk1DU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}