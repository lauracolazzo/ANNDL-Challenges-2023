{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1epMXh_zCs37zNw5Vjb1yS-2o48zeAexG","timestamp":1703249713120},{"file_id":"10J_az1_8-8k3_naP_myLNV21pd9Tz0AS","timestamp":1702427139054}],"gpuType":"T4","collapsed_sections":["xPJ6ZnUBvn6U","R4LoegHEvs4w","xUe_3cmzvzdk","fJmdi4Taflfp","YJw9UiMwnxXJ","EHN-Waxqn0gm"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Connect to Drive"],"metadata":{"id":"xPJ6ZnUBvn6U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-okJO0BvhuN"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/Path-to-Data/"]},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"R4LoegHEvs4w"}},{"cell_type":"code","source":["# Fix randomness and hide warnings\n","seed = 42\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(seed)\n","\n","import logging\n","\n","import random\n","random.seed(seed)"],"metadata":{"id":"V3gFi8fEvve3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import tensorflow\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","print(tf.__version__)"],"metadata":{"id":"VIGJeQ3rvxeq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","plt.rc('font', size=16)\n","from sklearn.preprocessing import MinMaxScaler"],"metadata":{"id":"TP5VwOjH52At"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load and process data"],"metadata":{"id":"xUe_3cmzvzdk"}},{"cell_type":"code","source":["training_data = np.load('training_data.npy')\n","valid_periods = np.load('valid_periods.npy')\n","categories = np.load('categories.npy')\n","\n","print(\"Training dataset shape: \", training_data.shape)\n","print(\"Valid periods shape: \", valid_periods.shape)\n","print(\"Categories shape:\", categories.shape)"],"metadata":{"id":"s64paexov2I-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(training_data)\n","print(training_data.dtype)"],"metadata":{"id":"1RGg1SJRxgbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data = training_data.astype(np.float32)\n","print(training_data.dtype)"],"metadata":{"id":"RUXeMjoS0Z62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(valid_periods)\n","print(valid_periods.dtype)"],"metadata":{"id":"OZfnF6cHxkFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_periods = valid_periods.astype(np.int32)\n","print(valid_periods.dtype)"],"metadata":{"id":"kdL8njlD0ljH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(categories)\n","print(categories.dtype)"],"metadata":{"id":"efeQAmZexnKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the count of samples for each category\n","category_labels, count = np.unique(categories, return_counts=True)\n","\n","# Print the list of category labels\n","print(\"Category labels: \", category_labels)\n","\n","# Print the count of samples for each category\n","for cat, count in zip(category_labels, count):\n","    print(f\"Category: {cat}, Number of samples: {count}\")\n","\n","dic = {'A':0,\n","       'B':0,\n","       'C':0,\n","       'D':0,\n","       'E':0,\n","       'F':0}\n","\n","\n","for row in range(0, len(valid_periods)):\n","  if valid_periods[row][1] - valid_periods[row][0] >= 220:\n","      dic[categories[row]]+=1\n","print(dic)"],"metadata":{"id":"bnkVyePJGP1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pfA6xF_7AkAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspect the timeseries' lengths\n","min_length = training_data.shape[1] + 1\n","max_length = 0\n","idx_min = 0\n","idx_max = 0\n","sum = 0\n","\n","for row in range(valid_periods.shape[0]):\n","  start = valid_periods[row][0]\n","  end = valid_periods[row][1]\n","  length = end - start\n","  sum += length\n","\n","  if length > max_length:\n","    max_length = length\n","    idx_max = row\n","  if length < min_length:\n","    min_length = length\n","    idx_min = row\n","  # print(f'Time series {row}, length: {length}')\n","\n","print(f'Minimum length: {min_length}, row index: {idx_min}')\n","print(f'Maximum length: {max_length}, row index: {idx_max}')\n","print(f'Average length: {sum/training_data.shape[0]}')"],"metadata":{"id":"zXbeucQjqfAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx_set = []\n","idx = 0\n","category_labels = category_labels.tolist()\n","\n","# Get the one row index (time series index) for each available category\n","while len(category_labels) != 0 & idx < training_data.shape[0]:\n","  category = categories[idx][0]\n","  if category in category_labels:\n","    idx_set.append(idx)\n","    category_labels.remove(category)\n","  idx += 1\n","\n","# Plot one time series per category, removing the padding\n","for i in idx_set:\n","  start = valid_periods[i][0]\n","  end = valid_periods[i][1]\n","  x_values = np.arange(end - start)\n","\n","  plt.figure(figsize=(11, 4))\n","  plt.plot(x_values, training_data[i][start:end])\n","  plt.title(f'Time Series from Category {categories[i][0]}')\n","  plt.xlabel('Timestamp')\n","  plt.ylabel('Values')\n","  plt.show()"],"metadata":{"id":"vow0stSO6LMV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the size of the test set\n","test_set_percentage = 0.1\n","test_size = int(test_set_percentage * training_data.shape[0])\n","\n","# Compute the number of samples for each category to be moved inside the test set\n","test_samples_per_cat = int(test_size / len(np.unique(categories)))\n","\n","print(\"Test size: \", test_size)\n","print(\"Samples per category: \",test_samples_per_cat)"],"metadata":{"id":"w20sqlyonE5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_samples_per_cat(categories, dataset, samples_per_cat, time_series_length):\n","\n","  rows_to_delete = []\n","  test_F_category_len = 30\n","\n","  training_set = np.copy(dataset)\n","  training_set_categories = np.copy(categories)\n","  training_set_indices = np.copy(valid_periods)\n","  unique_categories = np.unique(categories)\n","  test_len = (len(unique_categories)-1)*samples_per_cat + test_F_category_len #other categories can have a lot of samples in test set, category F is too short\n","  test_set_categories = [0] * test_len\n","  test_set_indices = [0] * test_len\n","  test_set = [[0] * time_series_length for _ in range(test_len)]\n","  print(test_len)\n","  i = 0\n","  for c in unique_categories:\n","\n","    if c == 'F':\n","      counter = test_F_category_len\n","    else:\n","      counter = samples_per_cat\n","\n","    for row in range (len(dataset)):\n","      if (counter == 0):\n","        break;\n","\n","      if categories[row] == c :\n","\n","        rows_to_delete.append(row)\n","        test_set_categories[i] = c\n","        test_set[i] = dataset[row]\n","        test_set_indices[i] = valid_periods[row]\n","        counter-=1\n","        i+=1\n","\n","  training_set = np.delete(training_set, rows_to_delete, axis=0)\n","  training_set_categories = np.delete(training_set_categories, rows_to_delete, axis=0)\n","  training_set_indices = np.delete(training_set_indices, rows_to_delete, axis=0)\n","  test_set = np.array(test_set)\n","  test_set_categories = np.array(test_set_categories)\n","  test_set_indices = np.array(test_set_indices)\n","\n","  return training_set, training_set_categories, training_set_indices, test_set, test_set_categories, test_set_indices"],"metadata":{"id":"6GPtwhP6BBqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_set, training_set_categories, training_set_indices, test_set, test_set_categories, test_set_indices = get_samples_per_cat(categories, training_data, test_samples_per_cat, training_data.shape[1])\n","print(training_set.shape)\n","print(training_set_categories.shape)\n","print(training_set_indices.shape)\n","print(test_set.shape)\n","print(test_set_categories.shape)\n","print(test_set_indices.shape)"],"metadata":{"id":"p_Ma6Ux3N8Cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["window = 200\n","stride = 5\n","telescope = 18"],"metadata":{"id":"faxHH4b-YIQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_sequences(dataset,dataset_valid_idx,dataset_categories,window,stride,telescope):\n","  new_dataset = []\n","  new_labels = []\n","  new_categories = []\n","\n","  for ts_idx in range(len(dataset)):\n","    start = dataset_valid_idx[ts_idx][0]\n","    end = dataset_valid_idx[ts_idx][1]\n","    ts_len = end - start\n","\n","    #if the sequence is shorter than the window + the telescope, we pad it\n","    padding_check = window+telescope-ts_len\n","\n","    if padding_check > 0:\n","      padding = np.zeros((padding_check), dtype='float32')\n","      ts_temp = np.concatenate((padding,dataset[ts_idx][start:end]))\n","    else:\n","      ts_temp = np.array(dataset[ts_idx][start:end])\n","\n","    #make the window slide by a stride quantity\n","    i=0\n","    counter = 0\n","    while(i+window+telescope <= len(ts_temp)):\n","      new_dataset.append(ts_temp[i:i+window])\n","      new_labels.append(ts_temp[i+window:i+window+telescope])\n","      new_categories.append(dataset_categories[ts_idx])\n","      i+=stride\n","\n","    #if by sliding at the last step, we surpassed the end of the sequence and missed some values at the end\n","    #go back and take a window that contains those values\n","    if(i+window+telescope > len(ts_temp) and (i+window+telescope - stride) < len(ts_temp)):\n","      new_dataset.append(ts_temp[len(ts_temp)-telescope-window:len(ts_temp)-telescope])\n","      new_labels.append(ts_temp[len(ts_temp)-telescope:len(ts_temp)])\n","      new_categories.append(dataset_categories[ts_idx])\n","\n","\n","\n","  new_dataset = np.array(new_dataset)\n","  new_labels = np.array(new_labels)\n","  new_categories = np.array(new_categories)\n","  return new_dataset, new_labels, new_categories"],"metadata":{"id":"CJLcwfkMYW7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train, train_categories = build_sequences(training_set,training_set_indices,training_set_categories,window,stride,telescope)\n","X_test, y_test, test_categories = build_sequences(test_set,test_set_indices,test_set_categories,window,stride,telescope)\n","dic = {'A':0,\n","       'B':0,\n","       'C':0,\n","       'D':0,\n","       'E':0,\n","       'F':0}\n","for c in train_categories:\n","  dic[c]+=1;\n","print(dic)\n","\n","dic = {'A':0,\n","       'B':0,\n","       'C':0,\n","       'D':0,\n","       'E':0,\n","       'F':0}\n","for c in test_categories:\n","  dic[c]+=1;\n","print(dic)\n","\n","print(X_train.shape, y_train.shape, train_categories.shape)\n","print(X_test.shape, y_test.shape, test_categories.shape)"],"metadata":{"id":"W3Tkb_yiQB-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = X_train.reshape(-1, X_train.shape[1], 1)\n","y_train = y_train.reshape(-1, y_train.shape[1], 1)\n","\n","print(X_train.shape, y_train.shape,)"],"metadata":{"id":"iRGHCp-3mOy2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build model\n"],"metadata":{"id":"fJmdi4Taflfp"}},{"cell_type":"code","source":["input_shape = X_train.shape[1:]\n","output_shape = y_train.shape[1:]\n","batch_size = 128\n","epochs = 200\n","\n","print(input_shape)\n","print(output_shape)\n"],"metadata":{"id":"O3iJinh6foa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model(input_shape, output_shape):\n","\n","\n","  input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n","  x = tfkl.LSTM(128, return_sequences=True, name='lstm1')(input_layer)\n","  x = tfkl.LSTM(128, name='lstm2',dropout = 0.4)(x)\n","\n","  output_layer = tfkl.Dense(output_shape[0],name='output_layer')(x)\n","\n","  model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='model')\n","\n","  model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(1e-3))\n","\n","  return model"],"metadata":{"id":"aWKLAyAPfxgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_model(input_shape, output_shape)\n","model.summary()\n","tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"],"metadata":{"id":"ZZfEhjDPf40k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train model"],"metadata":{"id":"YJw9UiMwnxXJ"}},{"cell_type":"code","source":["# Train the model\n","history = model.fit(\n","    x = X_train,\n","    y = y_train,\n","    batch_size = batch_size,\n","    epochs = epochs,\n","    validation_split=.2,\n","    callbacks = [\n","        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, min_delta= 0.0001 ,restore_best_weights=True),\n","        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n","    ]\n",").history"],"metadata":{"id":"sV_F7Zntf6_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_epoch = np.argmin(history['val_loss'])\n","plt.figure(figsize=(17,4))\n","plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n","plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.title('Mean Squared Error')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()\n","\n","plt.figure(figsize=(18,3))\n","plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()"],"metadata":{"id":"WyLuMvvfoOr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Predict the test set using the model\n","predictions = model.predict(X_test, verbose=0)\n","\n","#Print the shape of the predictions\n","print(f\"Predictions shape: {predictions.shape}\")\n","\n","#Calculate and print Mean Squared Error (MSE)\n","mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), predictions.flatten()).numpy()\n","print(f\"Mean Squared Error: {mean_squared_error}\")\n","\n","#Calculate and print Mean Absolute Error (MAE)\n","mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n","print(f\"Mean Absolute Error: {mean_absolute_error}\")"],"metadata":{"id":"uQtOfELoEPEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if isinstance(predictions, np.ndarray):\n","    print(\"Variable is a NumPy array\")"],"metadata":{"id":"t173PJxvIzxT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save model"],"metadata":{"id":"EHN-Waxqn0gm"}},{"cell_type":"code","source":["model.save('SubmissionModel')"],"metadata":{"id":"62gGU-ohn9QE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load model"],"metadata":{"id":"oszsPvCLn2TZ"}}]}